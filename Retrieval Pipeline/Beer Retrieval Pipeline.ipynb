{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Inital Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "model_package_group_name = \"PipelineModelPackageGroup\"\n",
    "pipeline_name = \"RetrievalPipeline\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 bucket pathing\n",
    "input_reviews_uri = \"s3://beer-reviews-models-pb/Rec Automation/Review Data/Initial Data/final_reviews.csv\".format(region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import ParameterInteger, ParameterString, ParameterFloat\n",
    "\n",
    "# Define the default pipeline parameters\n",
    "# These parameters can be easily changed when executing a pipeline\n",
    "\n",
    "# raw input data\n",
    "raw_s3_input = \"s3://beer-reviews-models-pb/Rec Automation/Review Data/Initial Data\".format(region)\n",
    "input_data = ParameterString(name=\"InputData\", default_value=raw_s3_input)\n",
    "\n",
    "\n",
    "processing_instance_count = ParameterInteger(name=\"ProcessingInstanceCount\", default_value=1)\n",
    "processing_instance_type = ParameterString(\n",
    "    name=\"ProcessingInstanceType\", default_value=\"ml.m5.xlarge\"\n",
    ")\n",
    "\n",
    "# status of newly trained model in registry\n",
    "# PendingManualApproval | Rejected | Approved\n",
    "model_approval_status = ParameterString(name=\"ModelApprovalStatus\", default_value=\"PendingManualApproval\") \n",
    "\n",
    "\n",
    "# training step parameters + hyperparameters\n",
    "training_instance_type = ParameterString(name=\"TrainingInstanceType\", default_value=\"ml.m5.2xlarge\")\n",
    "training_epochs = ParameterString(name=\"TrainingEpochs\", default_value=\"20\")\n",
    "training_recommendations = ParameterString(name=\"ReturnRecommendationsNumber\", default_value = \"500\")\n",
    "training_learning_rate = ParameterString(name=\"LearningRate\", default_value = \"0.5\")\n",
    "\n",
    "# model performance step parameters\n",
    "accuracy_threshold = ParameterFloat(name=\"AccuracyTop500Threshold\", default_value=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting retrieval_preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile retrieval_preprocessing.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def users_test_train_split(df, split_ratio=0.15):\n",
    "    split_len = int(len(df)*split_ratio)\n",
    "    test_df = df[:split_len]\n",
    "    train_df = df[split_len:]\n",
    "    \n",
    "    return test_df, train_df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_data_path_reviews = os.path.join(\"/opt/ml/processing/input/reviews\", \"final_reviews.csv\")\n",
    "    \n",
    "    train_output_path = os.path.join(\"/opt/ml/processing/train\", \"train.csv\")\n",
    "    test_output_path = os.path.join(\"/opt/ml/processing/test\", \"test.csv\")\n",
    "    \n",
    "    print(\"Reading review input data from {}\".format(input_data_path_reviews))\n",
    "    review_df = pd.read_csv(input_data_path_reviews, index_col=\"Unnamed: 0\")\n",
    "    \n",
    "    # Shuffle dataframe\n",
    "    review_df = review_df.sample(frac=1)\n",
    "    \n",
    "    # only get users with at least 5 reviews\n",
    "    users_with_favorable_ratings = (review_df['username'].value_counts()\n",
    "                                .loc[lambda x: x>10]\n",
    "                                .loc[lambda x: x<100]\n",
    "                                .index.values)\n",
    "    \n",
    "    review_df = review_df[review_df['username'].isin(users_with_favorable_ratings)]\n",
    "    \n",
    "    #Generate test train split\n",
    "    test_df, train_df = users_test_train_split(review_df)\n",
    "    \n",
    "    \n",
    "    print(\"Saving Train Data {}\".format(train_output_path))\n",
    "    train_df.to_csv(train_output_path, header=True, index=True)\n",
    "    \n",
    "    print(\"Saving Test Data {}\".format(test_output_path))\n",
    "    test_df.to_csv(test_output_path, header=True, index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "# create the processor for the job + pass in pipeline parameters\n",
    "\n",
    "sklearn_processing = SKLearnProcessor(\n",
    "    framework_version=\"0.20.0\", \n",
    "    role=role,\n",
    "    instance_type=processing_instance_type, \n",
    "    instance_count=processing_instance_count,\n",
    "    base_job_name = \"retrieval-data-processing\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define inputs and outputs for the job\n",
    "\n",
    "processing_inputs = [\n",
    "            \n",
    "            ProcessingInput(source=input_reviews_uri, destination=\"/opt/ml/processing/input/reviews\",\n",
    "                            input_name = \"input review data\"),\n",
    "        ]\n",
    "\n",
    "processing_outputs = [\n",
    "            ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/train\"),\n",
    "            ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/test\"),\n",
    "    ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the processing step\n",
    "\n",
    "PROCESSING_SCRIPT_LOCATION = \"retrieval_preprocessing.py\"\n",
    "#cache_config = CacheConfig(enable_caching=True, expire_after=\"PT1H\")\n",
    "\n",
    "processing_step = ProcessingStep(\n",
    "    \"ProcessData\", \n",
    "    processor = sklearn_processing,\n",
    "    inputs = processing_inputs,\n",
    "    outputs = processing_outputs,\n",
    "    code = PROCESSING_SCRIPT_LOCATION\n",
    "    #cache_config=cache_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tf_ret_train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tf_ret_train.py\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "\n",
    "from typing import Dict, Text\n",
    "import argparse\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# disable tf logging\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    # Hyperparameters- sent by client passed as command line args to script\n",
    "    parser.add_argument('--epochs', type=int, default=4)\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.5)\n",
    "    parser.add_argument('--returned_recommendations', type=int, default=500)\n",
    "    \n",
    "    # data directories\n",
    "    parser.add_argument(\"--train\", type=str, default=os.environ.get(\"SM_CHANNEL_TRAIN\"))\n",
    "    \n",
    "    # model directory - /opt/ml/model   by default for sagemaker\n",
    "    #parser.add_argument(\"--model_dir\", type=str)\n",
    "    parser.add_argument(\"--sm-model-dir\", type=str, default=os.environ.get(\"SM_MODEL_DIR\"))\n",
    "    parser.add_argument(\"--hosts\", type=list, default=json.loads(os.environ.get(\"SM_HOSTS\")))\n",
    "    parser.add_argument(\"--current-host\", type=str, default=os.environ.get(\"SM_CURRENT_HOST\"))\n",
    "    \n",
    "    return parser.parse_known_args()\n",
    "\n",
    "\n",
    "def get_train_data(train_dir):\n",
    "    print(\"training_directory\", train_dir)\n",
    "    df_train = pd.read_csv(os.path.join(train_dir, 'train.csv'), index_col=\"Unnamed: 0\")\n",
    "\n",
    "    print('x train: ', np.shape(df_train))\n",
    "    return df_train\n",
    "\n",
    "\n",
    "\n",
    "def df_to_tensor(df):\n",
    "    \n",
    "    df_beer = df['beer_name'].unique()\n",
    "    df_beer = pd.DataFrame(df_beer, columns = ['beer_name'])\n",
    "    \n",
    "    df_ratings = df[['username', 'beer_name']]\n",
    "    df_ratings = df_ratings.dropna()\n",
    "    \n",
    "    # convert dataframes to tensors\n",
    "    tf_beer_dict = tf.data.Dataset.from_tensor_slices(dict(df_beer))\n",
    "    tf_ratings_dict = tf.data.Dataset.from_tensor_slices(dict(df_ratings))\n",
    "    \n",
    "    # map rows to a dictionary\n",
    "    ratings = tf_ratings_dict.map(lambda x: {\n",
    "        \"beer_name\": x[\"beer_name\"],\n",
    "        \"username\": x[\"username\"]\n",
    "    })\n",
    "    beer_list = tf_beer_dict.map(lambda x: x['beer_name'])\n",
    "    \n",
    "    print('converted df to tensors')\n",
    "    return ratings, beer_list\n",
    "\n",
    "\n",
    "def get_unique_beers_and_users(ratings, beer_list):\n",
    "    usernames = ratings.map(lambda x: x['username'])\n",
    "    unique_users = np.unique(np.concatenate(list(usernames.batch(1000))))\n",
    "    unique_beers = np.unique(np.concatenate(list(beer_list.batch(1000))))\n",
    "\n",
    "    print(\"unique users: \", len(unique_users), \"unique_beers: \", len(unique_beers))\n",
    "    return unique_users, unique_beers\n",
    "\n",
    "    \n",
    "def test_train_split(ratings, df):\n",
    "    tf.random.set_seed(42)\n",
    "    shuffled = ratings.shuffle(len(df), seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "    train = shuffled.take(int(len(df)*0.8))\n",
    "    test = shuffled.skip(int(len(df)*0.8)).take(int(len(df)*0.2))\n",
    "    print(\"test data len: \", len(test), \"train data len: \", len(train))\n",
    "    return test, train\n",
    "    \n",
    "    \n",
    "# extend the tfrs class\n",
    "class BeerRetreival(tfrs.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        embedding_dims = 32\n",
    "        self.user_model =  tf.keras.Sequential([\n",
    "            tf.keras.layers.StringLookup(\n",
    "                vocabulary= unique_users, mask_token=None),\n",
    "            tf.keras.layers.Embedding(len(unique_users)+1, embedding_dims)\n",
    "        ])\n",
    "\n",
    "        self.beer_model = tf.keras.Sequential([\n",
    "            tf.keras.layers.StringLookup(\n",
    "                vocabulary=unique_beers, mask_token=None),\n",
    "            tf.keras.layers.Embedding(len(unique_beers)+1, embedding_dims)\n",
    "        ])\n",
    "\n",
    "        self.task = tfrs.tasks.Retrieval(\n",
    "                        metrics=tfrs.metrics.FactorizedTopK(\n",
    "                        candidates=beer_list.batch(128).cache().map(self.beer_model)\n",
    "                        ))\n",
    "        \n",
    "    \n",
    "    def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
    "        user_embeddings = self.user_model(features['username'])\n",
    "        beer_embeddings = self.beer_model(features['beer_name'])\n",
    "        return self.task(user_embeddings, beer_embeddings)\n",
    "        \n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    args, _ = parse_args()\n",
    "    \n",
    "    print('Training data location: {}'.format(args.train))\n",
    "    \n",
    "    df_train = get_train_data(args.train)\n",
    "    \n",
    "    ratings, beer_list = df_to_tensor(df_train)\n",
    "    unique_users, unique_beers = get_unique_beers_and_users(ratings, beer_list)\n",
    "    test, train = test_train_split(ratings, df_train)\n",
    "    \n",
    "    returned_recommendations = args.returned_recommendations\n",
    "    epochs = args.epochs\n",
    "    learning_rate = args.learning_rate\n",
    "\n",
    "    print('returned reccomendations = {}, epochs = {}, learning rate = {}'\n",
    "          .format(returned_recommendations, epochs, learning_rate))\n",
    "    \n",
    "    # create + train model\n",
    "    model = BeerRetreival()\n",
    "    optimizer = tf.keras.optimizers.Adagrad(learning_rate)\n",
    "    model.compile(optimizer)\n",
    "    model.fit(train.batch(8192),\n",
    "             validation_data = test.batch(512),\n",
    "             validation_freq = 2,\n",
    "             epochs = epochs,\n",
    "             verbose = 0)\n",
    "\n",
    "    # Eval model\n",
    "    scores = model.evaluate(test.batch(8192), return_dict=True, verbose=0)\n",
    "    print(\"top 100 score: \", scores['factorized_top_k/top_100_categorical_accuracy'])\n",
    "\n",
    "    #save model - need to call first\n",
    "    brute_force = tfrs.layers.factorized_top_k.BruteForce(model.user_model, k=500)\n",
    "    brute_force.index_from_dataset(\n",
    "        beer_list.batch(128).map(lambda beer_name: (beer_name, model.beer_model(beer_name)))\n",
    "    )\n",
    "\n",
    "    _ = brute_force(np.array([\"pblackburn\"]))\n",
    "    \n",
    "    if args.current_host == args.hosts[0]:\n",
    "        \n",
    "        print(\"Host arg:\", args.hosts[0])\n",
    "        # save model to an S3 directory with version number '01' in Tensorflow SavedModel Format\n",
    "        print(\"Saving Model to: \", args.sm_model_dir)\n",
    "        tf.saved_model.save(\n",
    "          brute_force,\n",
    "          os.path.join(args.sm_model_dir, \"01\"))\n",
    "        print(\"Saved Model to: \", args.sm_model_dir)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "tensorflow-recommenders\n",
    "pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "import time\n",
    "\n",
    "prefix = 'retrieval-model'\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "model_path = f\"s3://{bucket}/{prefix}/model/\"\n",
    "\n",
    "hyperparameters = {\n",
    "                   'epochs': training_epochs, \n",
    "                   'returned_recommendations': training_recommendations,\n",
    "                   'learning_rate': training_learning_rate\n",
    "                  }\n",
    "\n",
    "retrieval_estimator = TensorFlow(\n",
    "                            entry_point = 'tf_ret_train.py',\n",
    "                            dependencies=['requirements.txt'],                       \n",
    "                            instance_type = training_instance_type,\n",
    "                            instance_count = 1,\n",
    "                            hyperparameters=hyperparameters,\n",
    "                            role=sagemaker.get_execution_role(),\n",
    "                            framework_version='2.5',\n",
    "                            py_version='py37',\n",
    "                            base_job_name=\"tensorflow-train-model\",\n",
    "                            output_path = model_path\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE how the input to the training job directly references the output of the previous step.\n",
    "\n",
    "step_train_model = TrainingStep(\n",
    "    name=\"TrainRetrievalModel\",\n",
    "    estimator=retrieval_estimator,\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=processing_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"train\"\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\",\n",
    "        ),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting evaluate.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile evaluate.py\n",
    "\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    model_path = f\"/opt/ml/processing/model/model.tar.gz\"\n",
    "    with tarfile.open(model_path, \"r:gz\") as tar:\n",
    "        tar.extractall(\"./model\")\n",
    "\n",
    "    model = tf.saved_model.load(\"./model/01\")\n",
    "\n",
    "    test_path = \"/opt/ml/processing/test/\"\n",
    "    test_df = pd.read_csv(test_path+\"/test.csv\")\n",
    "    \n",
    "    # there is probably a better way to do this -\n",
    "    users = test_df['username'].unique()\n",
    "    accuracy = []\n",
    "    \n",
    "    for i in range(0, len(users)):\n",
    "        # Get predictions (as tensors)\n",
    "        _, beers = model(tf.constant([users[i]]))\n",
    "        # Convert tensors to numpy array\n",
    "        beer_list = [x.decode('UTF-8') for x in beers[0].numpy()]\n",
    "        # Get the users selected beers\n",
    "        true_list = test_df[test_df['username']==users[i]]['beer_name'].values\n",
    "        user_accuracy = np.isin(true_list, beer_list)\n",
    "        accuracy.append(np.count_nonzero(user_accuracy)/len(user_accuracy))\n",
    "        \n",
    "    top_500_accuracy = np.array(accuracy).mean()\n",
    "    print(\"Top 500 Accuracy: \", top_500_accuracy)\n",
    "\n",
    "    # Available metrics to add to model:\n",
    "    # https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-model-quality-metrics.html\n",
    "    report_dict = {\n",
    "        \"multiclass_classification_metrics\": {\n",
    "            \"accuracy\": {\"value\": top_500_accuracy, \"standard_deviation\": \"NaN\"},\n",
    "        },\n",
    "    }\n",
    "\n",
    "    output_dir = \"/opt/ml/processing/evaluation\"\n",
    "    pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    evaluation_path = f\"{output_dir}/evaluation.json\"\n",
    "    with open(evaluation_path, \"w\") as f:\n",
    "        f.write(json.dumps(report_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.properties import PropertyFile\n",
    "from sagemaker.sklearn.processing import ScriptProcessor\n",
    "\n",
    "tf_eval_image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"tensorflow\",\n",
    "    region=region,\n",
    "    version='2.5',\n",
    "    image_scope=\"training\",\n",
    "    py_version=\"py37\",\n",
    "    instance_type=training_instance_type,\n",
    ")\n",
    "\n",
    "evaluate_model_processor = ScriptProcessor(\n",
    "    role=role, \n",
    "    image_uri=tf_eval_image_uri, \n",
    "    command=['python3'], \n",
    "    instance_count=1, \n",
    "    instance_type=training_instance_type, \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PropertyFile\n",
    "# A PropertyFile is used to be able to reference outputs from a processing step, for instance to use in a condition step.\n",
    "# For more information, visit https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-propertyfile.html\n",
    "evaluation_report = PropertyFile(\n",
    "    name=\"EvaluationReport\", output_name=\"evaluation\", path=\"evaluation.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the evaluate_model_processor in a Sagemaker pipelines ProcessingStep.\n",
    "step_evaluate_model = ProcessingStep(\n",
    "    name=\"EvaluateRetrievalPerformance\",\n",
    "    processor=evaluate_model_processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=step_train_model.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "            destination=\"/opt/ml/processing/model\",\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=processing_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"test\"\n",
    "            ].S3Output.S3Uri,\n",
    "            destination=\"/opt/ml/processing/test\",\n",
    "        ),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"evaluation\", source=\"/opt/ml/processing/evaluation\"),\n",
    "    ],\n",
    "    code=\"evaluate.py\",\n",
    "    property_files=[evaluation_report],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "from sagemaker import PipelineModel\n",
    "\n",
    "tf_model_image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"tensorflow\",\n",
    "    region=region,\n",
    "    version='2.5',\n",
    "    image_scope=\"inference\",\n",
    "    py_version=\"py37\",\n",
    "    instance_type=training_instance_type,\n",
    ")\n",
    "\n",
    "tf_model = Model(\n",
    "    image_uri=tf_model_image_uri,\n",
    "    model_data=step_train_model.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    role=role,\n",
    ")\n",
    "\n",
    "pipeline_model = PipelineModel(\n",
    "    models=[tf_model],\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker_session\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "\n",
    "evaluation_s3_uri = \"{}/evaluation.json\".format(\n",
    "    step_evaluate_model.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"]\n",
    ")\n",
    "\n",
    "model_metrics = ModelMetrics(\n",
    "    model_statistics=MetricsSource(\n",
    "        s3_uri=evaluation_s3_uri,\n",
    "        content_type=\"application/json\",\n",
    "    )\n",
    ")\n",
    "\n",
    "step_register_pipeline_model = RegisterModel(\n",
    "    name=\"RegisterRetrievalModel\",\n",
    "    model=pipeline_model,\n",
    "    content_types=[\"text/csv\"],\n",
    "    response_types=[\"text/csv\"],\n",
    "    inference_instances=[\"ml.m5.xlarge\"],\n",
    "    transform_instances=[\"ml.m5.xlarge\"],\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    model_metrics=model_metrics,\n",
    "    approval_status=model_approval_status,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create condition step -  to check accuracy + conditionally register a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The class JsonGet has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.workflow.conditions import ConditionLessThanOrEqualTo, ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.workflow.condition_step import (\n",
    "    ConditionStep,\n",
    "    JsonGet,\n",
    ")\n",
    "\n",
    "# Models with a test accuracy lower 0.5 will not be registered with the model registry\n",
    "# I use the multiclass classification metric in the eval script \n",
    "# since I didn't see any metrics for retrieval models, but the metrics could be anything\n",
    "cond_lte = ConditionGreaterThanOrEqualTo(\n",
    "    left=JsonGet(\n",
    "        step=step_evaluate_model,\n",
    "        property_file=evaluation_report,\n",
    "        json_path=\"multiclass_classification_metrics.accuracy.value\",\n",
    "    ),\n",
    "    right=accuracy_threshold,\n",
    ")\n",
    "\n",
    "# Conditionally register the model, otherwise pass\n",
    "step_cond = ConditionStep(\n",
    "    name=\"Top-K-Greater-Than-Condition\",\n",
    "    conditions=[cond_lte],\n",
    "    if_steps=[step_register_pipeline_model], \n",
    "    else_steps=[],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the full pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "# Create a Sagemaker Pipeline.\n",
    "# The order of execution is determined from each step's dependencies on other steps,\n",
    "# not on the order they are passed in below.\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        processing_instance_type,\n",
    "        processing_instance_count,\n",
    "        training_instance_type,\n",
    "        input_data,\n",
    "        model_approval_status,\n",
    "        training_epochs,\n",
    "        training_recommendations,\n",
    "        training_learning_rate,\n",
    "        accuracy_threshold,\n",
    "    ],\n",
    "    steps=[processing_step, step_train_model, step_evaluate_model, step_cond],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit Pipeline to SageMaker + start execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#execution.wait()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
